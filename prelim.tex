\documentclass[oneside]{memoir}
\usepackage{microtype}
\usepackage{graphicx}
%\usepackage{subfigure}
\usepackage{booktabs} % for professional tables
\usepackage{subcaption}
\usepackage[all]{nowidow}
% \definecolor{grayish}{rgb}{0.95, 0.95, 0.96}
% \definecolor{corn}{RGB}{233, 210, 135}

% hyperref makes hyperlinks in the resulting PDF.
% If your build breaks (sometimes temporarily if a hyperlink spans a page)
% please comment out the following usepackage line and replace
% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
\usepackage{natbib}
\newcommand{\cicero}{\abr{Cicero} }
\newcommand{\amr}[1]{\texttt{#1}}
\usepackage{caption,color,fancyvrb}
%\usepackage{code,algorithmic,algorithm}
\usepackage{amsmath,amsthm,amsfonts}
\usepackage{algpseudocode}	% For pseudocode typesetting
\usepackage[colorlinks=true,linkcolor=blue]{hyperref}	% For hyper links
\usepackage{bbm}
\usepackage[T1]{fontenc}
% For CJK usage
\usepackage{CJKutf8} 
\usepackage{ucs} 
\usepackage[encapsulated]{CJK} 
\newcommand{\myfont}{gbsn}
\newcommand{\abr}[1]{\textsc{#1}}

% You need a newsubfloat element to use subcaption
\newsubfloat{figure}

\textwidth = 6.5 in
%\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
%\topmargin = 0.0 in
%\headheight = 0.0 in
%\headsep = 0.0 in
%\parskip = 0.2in
\parindent = 0.25in

\def\inv{^{-1}}


\begin{document} 

\begin{titlingpage}
\begin{center}

\textsc{\huge \bfseries {Enhancing Strategic Play with AI: }}\\[0.25cm]
\textsc{\huge \bfseries {Negotiations, Assistance, and Reinforcement Learning}}\\[0.25cm]
 
\emph{A prospectus submitted in partial fulfillment of the degree of Doctor of
Philosophy}\\[5.5cm]

%\textsc{\large DRAFT}\\
%\emph{October 18, 2011}\\[2.0cm]

\emph{Preliminary Oral Examination for}\\
\text{\large Wichayaporn Wongkamjan}\\[2.0cm] % [4.0cm]
\emph{Advisor:} \\
\text{Jordan Lee Boyd-Graber}\\[.5cm]
\emph{Committee Members:}\\
\textsc{<COMMITTEE MEMBER 1>}\\
\textsc{<COMMITTEE MEMBER 2>}\\
\textsc{<COMMITTEE MEMBER 3>}\\[2.0cm]

{\bfseries Department of Computer Science}\\
{\bfseries University of Maryland, College Park, MD 20742}\\
{\bfseries March 3, 2025}

\end{center}
\end{titlingpage}

\thispagestyle{empty}
\setcounter{page}{1}
\setcounter{secnumdepth}{3}

%\doublespacing
\DoubleSpacing

\begin{abstract}

Reinforcement Learning (RL) has been known as a crucial technique to solve decision-making problems. Decade studies mostly focus on optimality in discrete and continuous state-action spaces, maximizing sample efficiency or improving scalability. In recent years, we have witnessed RL in language space where it is first applied to align large language models (LLM) with human preferences. RL appears in a training step which requires human feedback collection to train a human reward model, giving \textit{human} feedback on language model token decisions. Recent studies have discovered that human alignment is not the best option when training LLM agents to solve reasoning tasks, e.g. Math (replace with real datasets). Without training any reward model, task rewards are enough to provide reward signals and improve LLM agents even further in reasoning tasks. 

Though, together with RL, LLM agents can perform as well as humans in multiple decision-making tasks (e.g. Go, Chess), their flaws still exist when there is a need for language for cooperation and social intelligence. Our goal is to explore in complex decision-making and language environments, exploit their (near-)optimal strategies while demonstrating their flaws in languages. Ultimately, we aim to enhance human and artificial intelligence (AI) interactions; improving language in AI to be as close to humans and utilizing AI strengths to guide humans in complex decision-making tasks. 

As AI improves, its strategy is yielding optimality, but a way to be as intelligent in language as humans is yet far. We show evidence that Cicero, the first Diplomacy AI agent that has been trained to be optimal in decision-making and a natural language model components, has been over-claimed that it is \textit{superhuman}. It sure does win over humans with high percentage, however, cooperation in such a competitive setting like Diplomacy is also a key. This requires persuasion and deception skills, in which AI is limited when compared to humans. We then introduce an AI advisory system, PHOLUS, designed to support human decision-making in Diplomacy, demonstrating that AI-generated strategic guidance can bridge the skill gap between novice and experienced agents. Lastly, we leverage counterfactual reinforcement learning to identify deception (CTRL-D) in negotiation. Though the work is under a toy setting like Diplomacy, deception under explicit state and action spaces is hard to be captured by humans, making this as well a challenging problem for AI.   

Building upon these foundational explorations in complex environments like Diplomacy, our proposed work will concentrate on significantly advancing AIâ€™s linguistic intelligence and its integration with strategic reasoning, aiming to improve AI capabilities while fostering human learning. We will enhance strategic AI agents by incorporating LLMs and developing an \textit{explainable} AI framework for strategic communication. Concurrently, we plan to develop an AI-driven strategic advisor capable of providing higher-level, abstract guidance, utilizing LLM fine-tuning process involving both supervised fine-tuning and reinforcement learning from human feedback with task-specific rewards. This research is expected to contribute a high-quality dataset annotated with  reasoning for strategic communication in Diplomacy and to experimentally validate methods for empowering LLMs to generate \textit{context-aware} strategic advice, yielding insights transferable to other domains requiring nuanced human-AI collaboration.

\end{abstract}
%\setstretch{.5}

\SingleSpacing

\newpage
\setcounter{tocdepth}{3}
\tableofcontents

\newpage
\listoffigures

\DoubleSpacing

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter Break
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Introduction}
\section{Motivation: The Evolving Frontier of AI in Complex Social and Strategic Domains}

Artificial Intelligence (AI) has made monumental impacts in navigating complex decision-making setups. From achieving superhuman proficiency in games of perfect information such as Chess \citep{campbell2002deepblue} and Go \citep{Silver2016MasteringTG}, recent research is increasingly deep into domains characterized by imperfect information, multi-agent interactions, and the indispensable role of human language. The advent and rapid evolution of Large Language Models (LLMs) \citep{Brown2020fewshot,Achiam2023GPT4TR} have unlocked capabilities in natural language understanding and generation, seemingly bridging the gap between computational intelligence and human-like communication. However, despite these advancements, the integration of sophisticated strategic reasoning with nuanced linguistic competence, especially in environments demanding social intelligence, remains a challenge.

Current AI systems, even those augmented with LLMs, often exhibit limitations when confronted with the subtleties of human cooperation, negotiation, persuasion, and deception \citep{Wei2022EmergentAO}. Issues such as ``hallucination'' \citep{Ji2022SurveyOH}, lack of robust common-sense reasoning, and brittleness in adversarial social contexts highlight that AI is yet to achieve true human-level parity in these complex communicative settings. This underscores a critical insight: while AI can excel in computationally intensive tasks, such as vast search planning or pattern recognition from massive datasets, its ``social and linguistic intelligence'' often lags. Consequently, rather than aiming for full autonomy in all scenarios, a more pragmatic and fruitful direction involves fostering human-AI cooperation, where AI's analytical strengths complement human intuition and social acuity.

This thesis is situated at this evolving frontier, seeking to explore and enhance AI's capabilities in environments where strategic decision-making is inextricably linked with complex linguistic interaction. The core motivation stems from the observation that while AI can learn optimal strategies in well-defined game-theoretic settings, its ability to effectively use language to achieve strategic goals--to persuade, to build trust, to negotiate, or even to ethically navigate deceptive information--is still in its early stages. This research aims to dissect these challenges, proposing novel frameworks and methodologies to imbue AI agents that already have sophisticated strategies with linguistic intelligence ability.

\section{Diplomacy: A Rich Testbed for AI in Strategic Communication}

To rigorously investigate the confluence of strategy, language, and social intelligence in AI, a suitable experimental environment is needed. The board game Diplomacy emerges as an exceptionally well-suited testbed for this purpose. It is a multi-agent game typically involving seven players, each controlling a major European power in the years leading up to World War I. The objective is to gain control of a majority of ``supply centers'' on the game map, which often necessitates forming and, crucially, sometimes betraying alliances.

Diplomacy's unique characteristics make it an ideal crucible for the research questions posed in this thesis:
\begin{itemize}
    \item \textbf{Multi-Agent Negotiation:} Unlike many games, Diplomacy has no dice or other elements of chance. Its core gameplay revolves around negotiation. In ``press'' variants of the game, players engage in private, free-form natural language communication with one another between game turns. This communication is the primary mechanism for forming alliances, coordinating actions, sharing information (or misinformation), and making threats or promises. The nature of this dialogue presents a significant challenge for AI language understanding and generation.
    \item \textbf{Simultaneous Move Resolution:} After a period of negotiation, all players secretly write down their orders for their units (e.g., move, support, hold, convoy). These orders are then revealed and resolved simultaneously. This mechanic means players must anticipate and counteract others' moves, often based on commitments or deceptions made during the negotiation phase. The success of a strategic plan is thus heavily dependent on the player's ability to predict others' actions, which are themselves influenced by linguistic interactions.
    \item \textbf{Mixed Cooperative and Competitive Dynamics:} While the ultimate goal is individual victory, no player can succeed alone in the early to mid-game. Alliances are essential. This creates a complex interplay of cooperation (within alliances) and competition (against those outside alliances, and eventually, even against former allies). Agents must balance the need to collaborate with the imperative to advance their own position, often leading to situations where trust is built and then strategically broken.
    \item \textbf{The Centrality of Persuasion and Deception:} Effective play in Diplomacy requires social skills. Players must be able to \textit{persuade} others to undertake actions beneficial to them, often by appealing to shared interests, reciprocity \citep{kramar2022negotiation}, or by framing proposals attractively \citep{Cialdini1993InfluenceTP}. Conversely, \textit{deception} is a common and often necessary tactic. This can range from subtle misdirection to outright lies and betrayal of established alliances \citep{peskov2020takes}. The ability for an AI to generate persuasive arguments, understand deceptive language, and make strategic decisions about when and how to be truthful or deceptive is a key focus.
    \item \textbf{Rich, Verifiable Data:} Games of Diplomacy generate a wealth of data: the sequence of board states, the orders submitted by each player, and, in press variants, transcripts of all messages exchanged. This allows for detailed post-hoc analysis of strategies, communicative acts, and their outcomes, providing a solid empirical basis for evaluating AI performance.
    \item \textbf{Tractability with Real-World Relevance:} While a game, Diplomacy models many aspects of real-world international relations, business negotiations, and other complex social interactions where strategic positioning and communication are key \citep{NEURIPS2019_84b20b1f}. Advances made in this ``toy'' yet deeply complex domain can offer valuable insights and transferable techniques for more applied settings.
\end{itemize}
The work of \cite{meta2022human} on Cicero, an AI agent claimed to achieve human-level play in Diplomacy by combining language models with strategic reasoning, serves as a significant benchmark and inspiration. However, as this thesis will explore, ``human-level play'' is multifaceted. While Cicero demonstrates strong strategic execution, the depth of its linguistic understanding, its ability to engage in truly human-like nuanced persuasion, or its capacity for deception remain areas ripe for further investigation and improvement. This thesis aims to push beyond current benchmarks, particularly in the integration of more profound linguistic intelligence with strategic reasoning.

\section{The Intricate Interplay: Reinforcement Learning and Language in Multi-Agent Strategic Settings}

The development of AI agents capable of mastering games like Diplomacy necessitates a deep integration of Reinforcement Learning (RL) for strategic decision-making and advanced Natural Language Processing (NLP) for communication. This section outlines the formalisms and challenges inherent in this interplay.

\subsection{Reinforcement Learning for Strategic Decision-Making in Multi-Agent Systems}

The foundational framework for single-agent sequential decision-making is the \textbf{Markov Decision Process (MDP)}. An MDP is formally defined by a tuple $(S, A, P, R, \gamma)$, where:
\begin{itemize}
    \item $S$ is a finite set of states.
    \item $A$ is a finite set of actions.
    \item $P: S \times A \times S \rightarrow [0,1]$ is the state transition probability function, where $P(s'|s,a)$ is the probability of transitioning to state $s'$ from state $s$ after taking action $a$.
    \item $R: S \times A \times S \rightarrow \mathbb{R}$ is the reward function, where $R(s,a,s')$ is the immediate reward received after transitioning from $s$ to $s'$ via action $a$.
    \item $\gamma \in [0,1]$ is the discount factor, balancing immediate versus future rewards.
\end{itemize}
The goal of an RL agent in an MDP is to learn a policy $\pi: S \rightarrow \Delta(A)$, a mapping from states to a probability distribution over actions, that maximizes the expected discounted cumulative reward, known as the value function. For a policy $\pi$, the state-value function $V^{\pi}(s)$ is defined as:
\[
V^{\pi}(s) = \mathbb{E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} | s_t=s \right]
\]
where $r_{t+k+1}$ is the reward at step $t+k+1$. The optimal policy $\pi^*$ maximizes $V^{\pi}(s)$ for all $s \in S$.

Diplomacy, however, is a multi-agent system. The appropriate formalism is thus a \textbf{Markov Game} \citep{Shapley1953StochasticG}. For $N$ agents, a Markov Game is defined by a tuple $(S, A_1, \dots, A_N, P, R_1, \dots, R_N, \gamma)$, where:
\begin{itemize}
    \item $S$ is the set of global states (e.g., the complete board configuration in Diplomacy).
    \item $A_i$ is the set of actions available to agent $i$. The joint action space is $A = A_1 \times \dots \times A_N$.
    \item $P: S \times A \times S \rightarrow [0,1]$ is the transition function, $P(s'|s, \mathbf{a})$, where $\mathbf{a} = (a_1, \dots, a_N)$ is a joint action.
    \item $R_i: S \times A \times S \rightarrow \mathbb{R}$ is the reward function for agent $i$.
    \item $\gamma$ is the common discount factor.
\end{itemize}
Each agent $i$ aims to learn a policy $\pi_i: S \rightarrow \Delta(A_i)$ that maximizes its own expected discounted return, given the policies of other agents $\pi_{-i}$. The value function for agent $i$ under a joint policy $\boldsymbol{\pi} = (\pi_1, \dots, \pi_N)$ is:
\[
V_i^{\boldsymbol{\pi}}(s) = \mathbb{E}_{\boldsymbol{\pi}} \left[ \sum_{k=0}^{\infty} \gamma^k r_{i, t+k+1} | s_t=s \right]
\]
RL algorithms for multi-agent settings (MARL) often involve concepts like Nash equilibria or aim to find best responses to other agents' policies \citep{Buoniu2008ACS}. Cicero, for instance, employs iterative algorithms like piKL-hedge, which regularizes policy updates towards an anchor policy (e.g., derived from human data) to maintain human-like behavior while optimizing for game performance \citep{meta2022human}. The piKL utility for agent $i$ is:
\[
\tilde{u}_{i,\lambda_{i}}(\pi_{i},\pi_{-i}) = \mathbb{E}_{a_i \sim \pi_i, a_{-i} \sim \pi_{-i}}[Q_i(s, a_i, a_{-i})] - \lambda_{i}D_{KL}(\pi_{i}||\tau_{i})
\]
where $Q_i(s, a_i, a_{-i})$ is the expected future reward for agent $i$ taking action $a_i$ when others take $a_{-i}$ in state $s$, and $\tau_i$ is the anchor policy. This formulation allows Cicero to make strong decisions based on the board state.

\subsection{The Challenge: Integrating Language into the RL Framework}

The primary challenge this thesis addresses is that in Diplomacy, the state $S$ is not just the board configuration. It is profoundly augmented by the linguistic interactions:
\begin{itemize}
    \item \textbf{Augmented State Space ($S'$):} A more complete state representation $s'_t \in S'$ must include not only the physical board state $s_t$ but also the history of communications $H_t = (m_1, m_2, \dots, m_k)$. Thus, $s'_t = f(s_t, H_t)$. Representing and updating these linguistic components of the state is non-trivial.
    \item \textbf{Augmented Action Space ($A'_i$):} An agent's action space $A'_i$ must also expand. Beyond physical moves $a_i$, it includes linguistic actions $m_{i \to j}$. A linguistic action $m_{i \to j} \in M$ could be sending a message $m$ with a specific communicative intent (e.g., proposing alliance, suggesting move, deceiving about intentions). The generation of $m$ itself is a complex sequential decision process at the token level, often guided by an LLM.
\end{itemize}
Cicero's architecture attempts this by having a strategic reasoning module (based on RL) that proposes desired future actions (an ``intent'' $z$), which then conditions a controllable dialogue model $p(m|s, z)$ to generate messages. However, this pipeline, while innovative, may not fully capture the bidirectional influence: language doesn't just enact pre-determined strategic intents; it actively shapes them and co-creates the strategic landscape. The ``language space'' is not only an output channel but an integral part of the decision-making environment. This thesis posits that current models like Cicero, while strong on strategy-based decisions, have significant room for improvement in leveraging $m_{i \to j}$ to navigate the richer state $S'$.

\subsection{The Critical Problem of Credit Assignment}

This leads to the crucial \textbf{credit assignment problem} \citep{10.5555/3312046}. In a long, multi-turn game like Diplomacy, if an agent wins, which actionsâ€”strategic or linguisticâ€”contributed to this victory? Was it a brilliant move three turns ago, a persuasive message sent at the start of the game that forged a key alliance, or a deceptive statement that misled a crucial opponent?
Consider a sequence of state-action pairs involving both board moves $(a)$ and linguistic actions $(m)$:
$s'_0 \xrightarrow{a_0, m_{i \to j,0}} s'_1 \xrightarrow{a_1, m_{i \to j,1}} \dots \xrightarrow{a_T, m_{i \to j,T}} s'_{T+1} \rightarrow \text{Outcome (e.g., Win/Loss)}$

Assigning credit $R_{total}$ back to the specific $m_{i \to j,t}$ that was pivotal is extremely difficult. The communicative actions create a complex, evolving social context that modulates the effectiveness of board actions. For example, the same proposed move $a$ might be accepted if preceded by trust-building dialogue ($m_{i \to j, t}$) but rejected if preceded by perceived deception ($\tilde{m}_{i \to j, t}$).
This thesis will investigate techniques to mitigate the credit assignment problem in such linguistically rich environments, potentially exploring:
\begin{itemize}
    \item \textbf{Reward Shaping:} Designing intermediate rewards based on linguistic cues (e.g., successful persuasion, detection of deception).
    \item \textbf{Human Preference and Task Rewards:} Incorporate LLM with preference finetuning, we hope to identify which parts of the dialogue that most influenced strategic outcomes and also align with human preference.
\end{itemize}
Understanding and improving how RL agents perform credit assignment in these intertwined strategic and linguistic spaces is a central aim of this research. We aim to develop agents that learn the strategic value of their utterances.

% diplomacy why? picture, how to play in short but mostly to motivating whyt diplomacy task are hard for AI

\section{What We Have Accomplished}
% slowly connect to research questions
% connect problems to what we've done and WHY we do that
Our initial research has focused on understanding the current capabilities and limitations of AI in complex strategic communication, using Diplomacy as our primary testbed. First, to establish a clear baseline, we conducted a rigorous benchmark of advanced AI, specifically examining models like \cicero. This investigation (Chapter~\ref{ch:benchmark_cicero}) assessed whether current AI can achieve human-like levels of persuasion and strategic deception in Diplomacy. The goal was to empirically identify specific areas where AI excels and where it still falls short of human performance in these critical communicative aspects, moving beyond general claims of ``human-level play'' to pinpoint concrete deficiencies.

Recognizing that AI, even with its limitations, possesses significant analytical strengths, we explored how these could benefit human players. We developed \textbf{P}ersonalized \textbf{H}elp for \textbf{O}ptimizing \textbf{L}ow-Skilled \textbf{U}sersâ€™ \textbf{S}trategy (PHOLUS, Chapter~\ref{sec:pholus}), an advisory system that integrates 
\cicero's strategic and linguistic outputs to provide decision support for novice Diplomacy players. This work leverages the \cicero's capacity for optimal searching, offering humans insights they might otherwise miss, thereby bridging human-AI collaboration to skill improvement.

Furthermore, we studied a challenge of deception in mixed cooperative-competitive environments. Human players often struggle to discern genuine cooperation from manipulative tactics. To address this, we conceptualized a framework for \textbf{C}oun\textbf{T}erfactual \textbf{\abr{rl}} against \textbf{D}eception (CTRL-D, Chapter~\ref{sec:ctrld}). This system identifies potentially deceptive proposals by framing them through a ``bait, switch and edge'' lens, similar to scam tactics. By utilizing Cicero's underlying value estimation model, augmented with a linear classifier, CTRL-D aims to create ``constructive friction,'' alerting human players to proposals that seem strategically too advantageous to be entirely truthful, thereby enhancing their critical assessment of communications. 

\section{What We Aim to Achieve}
% connect problems to what we will do and WHY
Building upon these foundational explorations, our future work will concentrate on significantly advancing AI's linguistic intelligence and its integration with strategic reasoning, with a dual focus on improving AI capabilities and fostering human learning.

Our primary objective is to enhance the linguistic sophistication of strategic AI agents. Current models like Cicero, while strong strategically, often rely on language generation pipelines that can be improved. We plan to systematically upgrade these linguistic components by integrating LLMs. Crucially, alongside this enhancement, we will develop a framework for explainable AI (XAI) specifically tailored for strategic communication in Diplomacy (further detailed in Chapter~\ref{ch:proposal}). This framework will allow the AI to articulate the reasoning behind its linguistic and strategic choices, a critical step for both debugging AI behavior and enabling human users to understand and learn from the AI's decisions.

Second, moving beyond mere decision suggestion, we aim to develop an AI-driven strategic advisor capable of providing higher-level, abstract guidance. This advisor will not just suggest moves or messages but will help users understand the strategic space: how to approach and manage alliances, balance long-term cooperative and competitive goals, and navigate the complex player intentions. To achieve this, we will employ a LLM fine-tuning process:
\begin{enumerate}
\item \textbf{Supervised Fine-Tuning (SFT)}: Current datasets like Metaâ€™s Diplomacy corpus contain rich gameplay data but lack strategic commentary. We propose using LLMs to generate advisory explanations alongside existing games, then refining them through human verification. This will create a new layer of interpretable data for training and evaluating AI that is an advisor. Ultimately, it bridges the gap between human decisions and the reasoning behind them.
\item \textbf{Reinforcement Learning from Human Feedback (RLHF) and Task-Specific Rewards}: We will align the LLM's outputs with human preferences for strategic advice and communicative nuance. Simultaneously, we will incorporate verifiable task rewards (e.g., successful alliance formation, improved game outcomes after following advice) to ground the LLM's reasoning to advice effectiveness.
\end{enumerate}
Through this approach, there are two key contributions: 1) a high-quality dataset annotated with human reasoning steps and preferences for strategic communication in Diplomacy, and 2) experimental validation that RLHF, combined with task-based rewards, can empower LLMs to generate sophisticated, context-aware strategic reasoning and advice. While initially demonstrated in Diplomacy, we believe these methodologies and the resulting insights will be transferable to other complex domains requiring nuanced strategic communication and human-AI collaboration.


% \section{Roadmap}

% introduce ... and provide background
% rl helps ...
% evidence that ai lacks
% ai can help ... and detect deception to create friction helpful!
% proposed solutions with 

% % Proposed Solutions}
% sota llm to help language in Cicero - smaller gap
% extend to other complex tasks like to help human improves its strategic abilities - new ways to discover 





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter Break
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Background}
\label{ch:background}
\section{Markov Games}
Markov Decision Processes (MDPs) are a formalization of sequential decision making, where actions influence rewards and subsequent states. The decision maker is an agent which interacts with an environment in a discrete time step, $t = 0,1,2,3, ...$. At each time step $t$, the agent observes a representation of the environment, state $s_t \in S$, and selects an action, $a_t \in A$. The agent receives a reward $r_t \in R$ and result in a new state $s_{t+1}$. 

MDPs is a setup for only one agent, while this work focuses on multiagent. We consider \textit{Markov game} \citep{Shapley1953StochasticG} in case of $n>1$.

\textbf{Definition.} \citep{bakhtin2023mastering}. An \textit{n}-agent Markov game $\Delta$ is a tuple $(S, A_1, \ldots, A_n, r_1, \ldots, r_n, P)$ where $S$ is the state space, $A^i$ is the action space of agent $i$ $(i = 1, \ldots, n)$, $r_i: S \times A_1 \times \cdots \times A_n \rightarrow \mathbb{R}$ is the reward function for agent $i$, and $f: S \times A_1 \times \cdots \times A_n \rightarrow S$ is the transition function.

The objective of each agent $i$ is to select a policy $\pi_i(s) : S \rightarrow \Delta A_i$ that maximizes its expected reward, given the policies of the other agents. At each state $s \in S$, agent $i$ simultaneously chooses an action $a_i$ from its action set $A_i$, while all other agents choose actions denoted collectively by $\mathbf{a}_{-i}$. Rather than selecting a single action deterministically, an agent may instead use a probability distribution over its actions. In this case, the probability of choosing action $a_i$ in state $s$ is given by $\pi_i(s, a_i)$, and the full action distribution is represented by the vector $\pi_i(s)$ or simply $\pi_i$.

\section{Diplomacy}
\textit{Diplomacy} is a strategic board game that combines negotiation
and strategy, where players take on the roles of various European
powers (nations) on the eve of World War~I.
%
The essence of the game lies in forming and betraying alliances to
control territories, requiring adept diplomacy (hence the name of the game) and strategic planning.

Some players focus on aggressive tactical decisions, while others
focus on making alliances, communicating, and collaborating with
others for better outcomes~\citep{pulsiphergames}.
%
The goal of the game is to capture territory, board regions called \textbf{supply centers}. Once you capture enough of these supply centers, you win the game.

The charm and challenge of \textit{Diplomacy} messages is that players
are free to talk about anything, either strategy-related or not. Because the game is relatively balanced between seven players at the
start, players need to form alliances if they hope to gain an advantage.
%
However, these alliances should be mutually beneficial; from a
player's perspective, they need to advocate for cooperation that 
benefits themselves.
%
This requires effective \emph{persuasion}~\citep{Cialdini1993InfluenceTP}: making
appeals to scarcity, reciprocity~\citep{kramar2022negotiation}, unity, or shared norms.
%
This is a communicative task which involves social and emotional skill: picking the right moves and convincing other players to help them.

However, the ultimate goal of \textit{Diplomacy} is for \emph{individual} players to
win the game.
%
This means that alliances will fall apart, leading to \emph{deception}~\citep{peskov2020takes}
as part of a betrayal~\citep{Niculae:Kumar:Boyd-Graber:Danescu-Niculescu-Mizil-2015}.
%
%
Because a player might benefit from a victim thinking that they are
working together, a betrayer often sets up the
tactical conditions for a betrayal while obfuscating their goals through cleverly composed deceptive messages (even if not outright lies).

\subsection{Deception}
Real-world deception manifests in various forms, such as \textit{scams} and \textit{phishing} attacks, where perpetrators exploit \textbf{trust} to manipulate victims into believing in the possibility of good fortune, even if it is unlikely~\citep{button2014online,muscanell2014weapons,hanoch2021scams}. These deceptive tactics often rely on persuasive language. If victims fall for these \textit{too good to be true} claims, they become targets and may comply with the perpetrators' requests---for example, disclosing sensitive information or making financial investments under false pretenses---ultimately resulting in monetary \textit{loss} or data breaches~\citep{burnes2017prevalence,coluccia2020online}. Those scammers would \textit{gain value} through data breaches or simply by acquiring cash.

Detecting deception remains a persistent challenge, especially when it is needed for real-world problems. We can use \abr{ai}, but it is hard to evaluate and requires quality feedback from humans to train models in detecting real deception. 
%
Deception in a limited space like a strategic game, e.g., Diplomacy, where nuanced persuasion and deception are required for winning, is more tractable to evaluate.
%
A bounded example would allow us to measure the ability of an AI to improve in deception detection.

% The geopolitical definition of deception is to manipulate
% adversaries' perceptions to gain strategic
% advantage~\citep{deception1982}. 

\section{Cicero: a Human-level Diplomacy agent}
Many research studies applied Markov games properties to Diplomacy where reinforcement Learning with self-play has been extensively used to train \abr{ai} agents in optimizing decision-making to solve strategic planning in Diplomacy~\citep{NEURIPS2019_84b20b1f,anthony2020learning,gray2020human,bakhtin2021nopress}. However, these models did not have ability in \textit{press}, where in Diplomacy, player can send a private message to others. Luckily, the most recent model, 
\cicero~\citep{meta2022human}, which represents the best AI agent that can play Diplomacy and has reached human level in terms of strategy and \textit{natural language} (where prior agents never succeeded before). 

\cicero is a result of cooperated models altogether to achieve complex tasks like Diplomacy. Generally, the objective of the game is to win, however, to win in this game is difficult,~\cicero needs to have ability for collaboration and betrayal during communication. As shown in~\cite{meta2022human}, \cicero has two main components corresponding to its strategy and communication parts. Since this thesis proposal mostly focus on dialogues of~\cicero; to understand and to improve, we want to provide details for dialogue generation while covering strategy decision models only at a high level. 

\subsection{Controllable Dialogue Model}
A key aspect of~\ciceroâ€™s dialogue generation is that it can be controlled by a game action. \cite{meta2022human} trained a conditional dialogue model, i.e., they learned the distribution~$p(x|y,z)$, where $z$ is some desired controllable attribute which can be any function of~$(x, y)$; as follows their S2 equation:

\begin{equation}
\mathcal{L}^{(i)}_{\text{MLE}}(p_\theta, \mathbf{x}^{(i)}, y^{(i)},  \mathbf{z}^{(i)}) = - \sum_{k=1}^{|y^{(i)}|} \log p_\theta\left(y^{(i)}_k \mid \mathbf{x}^{(i)}, y^{(i)}_{<k}, \mathbf{z}^{(i)}\right)
\label{eq:intent}
\end{equation}
At inference time, then, $z$ becomes a point of control over generation (i.e., intent as a control code), and can be set by sampling from a model $p(z^{(i)} | x^{(i)})$ or any other procedure. 

They define \textit{intent}: ``the most likely actions that the sender and recipient will take if no further dialogue occurs''. To learn a function \[
f(\mathbf{x}^{(i)}, \mathbf{y}^{(i)}) \approx \mathbf{z}^{(i)}
\] that can predict intent $\mathbf{z}^{(i)}$ out of a state~$\mathbf{x}^{(i)}$ and a dialogue~$\mathbf{y}^{(i)}$ given a game state, they trained a dialogue-conditional action prediction models. BART-large~\cite{} model was finetuned using \abr{webdiplomacy} dataset to output string representations of actions, e.g.:
\begin{verbatim}
F1901M
A APU H; A VEN H; F ION TUN
W1901A
F NAP B
S1902M
A APU H; A VEN TRI; F NAP ION; F TUN H
\end{verbatim}
which represents a sequence of actions of Italy that will take in the current and next several turns. They also injected ``agreement messages'' (i.e. I've entered those orders) to and from sender/recipient to signal the model that there is agreement between the two players.

With intent model $P(z^{(i)} \mid x^{(i)})$, a controllable language model can be trained as follows:
\[
P(y^{(i)} \mid x^{(i)}) = \sum_{z^{(i)}} P(y^{(i)} \mid x^{(i)}, z^{(i)}) \, P(z^{(i)} \mid x^{(i)}).
\]

To generate a message, there is a list of intent candidates where one is sampled through an intent model $P(z^{(i)} \mid x^{(i)})$, and others are sampled from piKL (which we will discuss next, Section~\ref{{sec:cicero_reason}}). piKL will assign high likelihood to (a) actions that the player likely to play given the dialogue, and (b) actions that are beneficial for the recipient. Therefore, the final intent will be selected considering human-like as well as suggesting \textit{better} moves for the recipient. 

Since language models generally suffer from hallucination or generate factually incorrect information, \cicero has a message filtering process that models as discriminators were trained and used at test-time to filter messages with mistakes. Note that there are more details of \cicero language model, we only cover important parts that would suffice for this thesis proposal.

% language model of cicero
% imitation language model
% intent prediction
% intent- controlled and how to select intent
\subsection{Strategic Reasoning Methods}
\label{sec:cicero_reason}

\textbf{Hedge}, introduced by Littlestone \& Warmuth (1994) and extended by Freund \& Schapire (1997), is an iterative algorithm that converges to an equilibrium. Suppose agent~$i$ selects an action~$a_i$ while all other agents choose actions~$\mathbf{a}_{-i}$. The agent receives a reward $u_i(a_i, \mathbf{a}_{-i})$, where~$u_i$ is derived from a reinforcement learning-trained value function. We define the average reward in hindsight for action~$a_i$ up to iteration~$t$ as:
\[
Q^t(a_i) = \frac{1}{t} \sum_{\tau \leq t} u_i(a_i, \mathbf{a}_{-i}^\tau).
\]
At each iteration $t$, the policy $\pi_i^t(a_i)$ is updated using the following softmax rule:
\[
\pi_i^t(a_i) \propto \exp\left( \frac{Q^{t-1}(a_i)}{\kappa_t} \right),
\]
where $\kappa_t$ is a temperature parameter that controls the exploration-exploitation trade-off.

It has been shown that setting $\kappa_t = \frac{1}{\sqrt{t}}$ ensures that, as $t \rightarrow \infty$, the \textit{average} policy across iterations converges to a coarse correlated equilibrium. In practice, this often approximates a Nash equilibrium closely.

The piKL algorithm, proposed by~\cite{bakhtin2023mastering}, aims to model human behavior (like behavioral cloning) while acting more optimally by incorporating KL-regularization during planning. \textbf{piKL} is a variant of hedge called piKL-hedge. Each agent $i$ seeks to maximize expected reward, while at the same time playing \textit{close} to an \textbf{anchor policy} $\tau_i$. These two goals define a composite utility function that adds a penalty of a \textit{distance} between an agent policy (that aims for optimality) and its anchor policy (that clones for human actions).

For each agent $i$, a utility function of the agent policy $\pi_i \in \Delta(A_i)$ given other agents' policies $\mathbf{\pi}_{-i}$:

\[
\tilde{u}_{i,\lambda_i}(\pi_i, \pi_{-i}) = u_i(\pi_i, \pi_{-i}) - \lambda_i D_{\mathrm{KL}}(\pi_i \parallel \tau_i),
\]

where $u_i$ is the expected reward function and $\lambda_i$ is a regularization coefficient scaling the penalty of a distance between an agent policy and a human anchor policy.

This results in a KL-regularized variant of the Hedge algorithm. At each iteration $t$, the agent's policy $\pi_i^t(a_i)$ is updated as:

\[
\pi_i^t(a_i) \propto \exp\left( \frac{Q^{t-1}(a_i) + \lambda_i \log \tau(a_i)}{\kappa_{t-1} + \lambda} \right).
\]

When $\lambda$ is large, the utility function is dominated by the term $\lambda_i D_{\mathrm{KL}}(\pi_i \parallel \tau_i)$, making the agent to tend to play a policy $\pi_i$ close to the anchor policy $\tau_i$. When $\lambda$ is small, the term $u_i(\pi_i, \pi_{-i})$ dominates and the agent tends to maximize reward without playing as closely to the anchor policy $\tau_i$.
 
% strategy beyond DiL-piKL (co-shar)
Problems from limited data and self-play RL still appear in piKL, where value models $V(s)$ suffer from high variance and though self-play RL may help with repeated games to learn high-quality policy and value models, \cite{bakhtin2021nopress} found that self-play RL from scratch performed poorly in Diplomacy when playing with humans. \textbf{CoShar piKL} was an answer to the problems above, to keep a policy close to human behaviors during optimization as well as modeling cross-player correlations in a joint policy. CoShar piKL iteratively improves a policy $\pi^t$ based in adjusting a joint anchor policy $\tau$ using the average expected value of policies on past iterations. Here are updated $Q^t$ and $\pi^t$ by~\cite{bakhtin2021nopress}:

\begin{align}
    Q^{t-1}_i(a_i) &= \mathbb{E}_{a_{-i} \sim \pi^{t-1}_{-i}(a_{-i})} V_i(a)  \\
    \pi^{\Delta t}(a) &\propto \tau(a) \exp\left( \sum_i Q^{t-1}_i(a_i)/\lambda \right)  \\
    \pi^t &= \left(\frac{t-1}{t}\right)\pi^{t-1} + \left(\frac{1}{t}\right)\pi^{\Delta t}
\end{align}
where expected values are given by a value function $V$ such that $V_i(a)$ is the value to player $i$ of the game state resulting from playing joint action $a$, and the anchor strength $\lambda$ is a degree to which the policy can deviate from $\tau$.

CoShar piKL is hard to compute due to the combinatorial action space where each player can have 35 actions to choose from, making the joint action space infeasible to compute. They simplified this with Correlated Best Response. Given a policy $\pi_{\theta_t}$ parametrized by a neural network $\theta_t$, Correlated Best Response (Cor-BR) perform a single-step of CoShar piKL to improve the policy as follows:

\begin{align}
    Q_i(a_i) &= \mathbb{E}_{a_{-i} \sim \pi_{-i}(a_{-i})} V_i(a), \\
    p(a) &\propto \tau(a) \exp\left( \sum_i Q_i(a_i)/\lambda \right). 
\end{align}
Here $p$ is analogous to $\pi^{\Delta t}$ from CoShar piKL, it is the policy to take a small step towards in order to improve the current policy $\pi_{\theta}$.

Then, they sample from $p$ using a self-normalized importance sampling. They sample with weight ratio $p(a)/q(a)$ where $q$ is the product of the independent piKL policies of each player.

\begin{align*}
    \hat{Q}_i(a_i) &:= \mathbb{E}_{a_{-i} \sim \pi_{\theta_{-i}}(\cdot \mid a_i)} V_i(a) \\
    \hat{q}(a) &:= \left( \prod_i \tau(a_i) \right) \exp\left( \sum_i \hat{Q}_i(a_i)/\lambda \right) \\
    \hat{p}(a) &:= \tau(a) \exp\left( \sum_i \hat{Q}_i(a_i)/\lambda \right) \\
    a^1, \ldots, a^{N_q} &\sim \hat{q}(\cdot) \\
    \hat{p}(a) &:= \hat{p}(a)/\hat{q}(a)\mathbb{I}(a \in \{a^1, \ldots, a^{N_q}\}) \\
    a^1, \ldots, a^{N_p} &\sim \hat{p}(\cdot)
\end{align*}

% LLM setting? rlhf and rl for reasoning
\section{Grounding Texts}
We provide background of Diplomacy and \cicero, yet to understand how well humans and \cicero communicate, we need a tool to extract abstract information that appears in messages. A tool in most of our works is Abstract Meaning Representation~\citep[\abr{amr},][]{banarescu-etal-2013-abstract}. It captures ``\textit{who is doing what to whom}''. Each sentence can be represented with a rooted, directed, acyclic graph with labels on edges (relations) and leaves (concepts). Given an example:

\begin{figure}[h]
    \small{ \texttt{(w / want-01\\
    \hspace*{4 mm}:ARG0 (b / boy)\\
    \hspace*{4 mm}:ARG1 (b2 / believe-01\\  
    \hspace*{8 mm}:ARG0 (g / girl)\\
    \hspace*{8 mm}:ARG1 b)),}}
\end{figure}
where there is a wanting event, whose \texttt{ARG0} (wanter) is a boy, and whose \texttt{ARG1} (wanted) is a believing event. The believing event has an \texttt{ARG0} (believer), which is a girl, and has \texttt{ARG1} (believed) as the same boy. The variables \texttt{w}, \texttt{b}, \texttt{b2} and \texttt{g} correspond to internal nodes in the graph. Note that b appears twice, which the first time as \texttt{b / boy} and second time is just \texttt{b}. The AMR above can be interpreted in English with various sentences:
\begin{itemize}
    \item The boy wants the girl to believe him.
    \item The boy desires to be believed by the girl.
    \item The boy has a desire to be believed by the girl.
\end{itemize}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter Break
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \chapter{Reinforcement Learning towards Feedback Alignment}
% \label{ch:rl}

% connect rlhf back to model-based rl
% why model-based rl?
% we work on a framework to make dynamics model (learn rt and st+1) to best learn from state action historical data by adjusting historical policy distribution. 

% though this is prior llm era that has not involves human feedback yet,  we see connection that ...

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter Break
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Benchmarking AI Strategy and Social intelligence}
\label{ch:benchmark_cicero}
% motivation
% why diplomacy -> cicero
A board game that combines negotiation and strategy, Diplomacy serves as a right fit where 7 agents take on the roles of European powers on the eve of World War I within a mixed cooperative and competitive environment. As a strategy game, Diplomacy requires strategic planning not only about the moves (like Chess, Go) but also about negotiation to form alliances, or even break alliances with betrayal. We focus on deception as part of betrayal, that though it happens rarely (5\%, Peskov 2020), it is the most essential to capture since it could cost the victim to lose control of territories while the deceiver can gain more territories.

With these elements of strategy and negotiation of Diplomacy, \cicero, an AI agent is claimed to be \textit{superhuman}, surpassing human agents and has \textit{mastered Diplomacy}. Although this is a truly breakthrough, however, \cicero has been overclaimed since it is only showing that it is winning more than humans, while aspects of negotiations to form alliance, to persuade such moves that are beneficial for both parties or to deceive when breaking alliances, have not been well established. These communicative acts are necessary for mastering and enjoying Diplomacy and grounded in the state of the game (i.e. unit moves are mentioned within persuasive and deceptive messages). We see this opportunity to measure whether \cicero is human-level in both strategy and communication as claimed.

\section{Intentions to Persuasion and Deception}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figures/dec_per_motivating_example.pdf}
    \caption{Our goal is to detect when agents use persuasion and deception and compare human agents to \cicero. First, we retrieve initial orders (left), then extract
      moves from natural language communication (middle) through \abr{amr}
      \ (Section~\ref{sec:amr}), and
      later detect deception and persuasion
      (Section~\ref{sec:detection}) conditioning initial intents and
      final orders (right). We show two possibilities: (top)
      Germany breaks its commitment to England by moving to Norway
      instead of Sweden, and (bottom) England successfully persuades
      Germany if Germany moves its unit to Sweden as England suggests
      and this move is not in Germany's initial orders.}
    \label{fig:dec_per_example}

\end{figure*}

Consider this in-game statement:
\begin{quote}
    You can steal STP [St. Petersburg] from Russia if youâ€™re in SWE [Sweden] next turn. I will support you there.
\end{quote}
which was made by England to Germany about a move from Sweden to St. Petersburg and shows a will to support. Ultimately, we want to be able to tell if the speaker is lying and if the speaker is able to convince the recipient to alter their action. This is necessary to measure how effectively Cicero communicates in this game. 

To understand communication with Diplomacy, we categorize intentions into three as shown in Figure X. For each turn of the game, the first intent before any communication is an initial intent. During communication, a speaker can express their intent through text, which we call communicative intent. In the final intent, we just call it as the final order which is the order (unit moves) the agent submits to the game. The initial intent and final order is easy to capture through the game engine (Section X). However, the communicative intent that is expressed through a message from the speaker to the recipient is hard to capture into an exact order. Thus, we use Abstract Meaning Representation (\abr{amr}, cite) to build a machine-readable representation of the intentions within communication. We describe annotations and training \abr{amr} parser more in Section X.

In the example, England says it would support Germany's move into Sweden from Skagerrak Sea, while Germany agrees with the proposal from England. To decide whether this is deceptive or persuasive. We first look into their definitions, where deception can be broken down into two subconcepts: breaking of a commitment and lying. We say that Germany violates a commitment with England if Germany verbally agrees to move to Sweden but actually attacks England in Norway. For persuasion, Englandâ€™s request is
considered persuasive if Germany moves to Sweden, as England suggests, instead of Germanyâ€™s
original plan to move to the North Sea. We describe each of these
more formally in Section X.

%method and its goal
\subsection{Grounding Messages into Intentions}
We already introduced \abr{amr} as to build a human-readable representation of the communicative intents, now we want to dive deep into how we make use of it to translate any English message to a speaker's intentions. First, we focus on annotating a set of predicates that are critical to Diplomacy (e.g. ally, move, originally from \abr{DAIDE}) to encode intents within communication. Diplomacy has its unique vocabulary (an example in the above statement). We extend \abr{amr} vocabulary to include provinces like SWE for Sweden and verbs like threaten and demilitarize, as well as to describe actions like gaining or holding. In the annotation phase, we have Diplomacy experts annotate sentences from Peskov to train human annotators. We annotate 8,878 utterances in  total where these are further used for training our English-to-\abr{amr} parser to extract communicative intent information from any messages.

We finetune a sequence-to-sequence model on \abr{amr} 3.0 dataset (cite X) as a baseline model and then we further finetune this baseline model on Diplomacy \abr{amr}-annotated, where this improves \abr{SMATCH} from 22.8 to 61.9. With additional steps like providing countries of speaker and recipient, replacing pronouns with country and  replacing provinces in abbreviation with their full names, help increasing \abr{SMATCH} to 66.6. This parser enables us to capture communicative intents which are essential in detecting deception and persuasion. 

\subsection{Broken Commitment and Persuasion}
We break deception into two subconcepts where we mostly focus on broken commitment, as in the above example, Germany agrees to move to Sweden but actually attacks England in Norway. Capturing when a commitment is broken is possible through intents whereas capturing lies is challenging. As studied by Peskov that human agents annotate messages as either truthful or deceptive (while playing), they define deception to agents: \textit{Typically, when [someone] lies [they] say what [they] know to be false in an attempt to deceive the listener}. This definition of lies is broader than a broken commitment, while the two still lie under deception. With this, we formulate a broken commitment as a partial form of deception when a agent~$i$ commits to doing an
action~$a^{i\to j}_{\text{msg}}$ and does not do it. In other words, given a set of \texttt{final orders}~$\mathbf{A}^{i}_{\text{final}}$ from agent $i$, 
if $a^{i\to j}_{\text{msg}} \notin \mathbf{A}^{i}_{\text{final}}$, then this is a broken commitment, i.e., 
\begin{equation}
    \text{BC}(a^{i\to j}_{\text{msg}},\mathbf{A}^{i}_{\text{final}}) = 
    \begin{cases}
        1,      & \text{if } a^{i\to j}_{\text{msg}} \notin \mathbf{A}^{i}_{\text{final}}\\
        0,      & \text{otherwise.}
    \end{cases}
    \label{eq:bc}
\end{equation}
Note that a agent~$i$ agreeing to 
agent~$j$'s \textit{proposal} to do
action~$a^{i\to j}_{\text{msg}}$ is equivalent to directly committing to \textit{doing} that
action.

Broken commitment is in some ways easier to detect than persuasion, as we are
only comparing a spoken intent to a final action. Persuasion is more difficult because we must discover initial
intents, then compare them to communication \emph{and} to final moves. Thanks to our game engine asking human agents to submit their initial plan of order before any communication, this helps us to be able to extract initial intents from humans. For \cicero, it cooperates between its strategic and language model using an order as intent of communication. Thus, communicative intent from \cicero can be generated before any communication. 


Persuasion happens when agent~$i$ talks to agent~$j$, suggests an
action~$a^{i\to j}_{\text{msg}}$, and then agent~$j$ makes a set of
\texttt{final orders}~$\mathbf{A}^{j}_{\text{final}}$ that is different from 
their \texttt{initial intents}~$\mathbf{A}^{j}_{\text{intent}}$. In other words, agent~$j$ is persuaded by agent~$i$ if they commit an action suggested by  agent~$i$, $a^{i\to j}_{\text{msg}} \in \mathbf{A}^{j}_{\text{final}}$ that was not agent~$j$'s initial intent $a^{i\to j}_{\text{msg}} \notin \mathbf{A}^{j}_{\text{intent}}$.
%
We define persuasion 
\begin{equation}
     % \text{Per}(\mathbf{A}^{j}_{\text{intent}}, a^{i\to j}_{\text{msg}}, \mathbf{A}^{j}_{\text{final}})
     \text{Per}(\mathbf{A}^{j}_{\text{intent}}, a^{i\to j}_{\text{msg}}, \mathbf{A}^{j}_{\text{final}}) = \\
    \begin{cases}
    1, & 
    \begin{aligned}[c]
    &\text{if } a^{i\to j}_{\text{msg}} \in \mathbf{A}^{j}_{\text{final}} \\
    &\text{and } a^{i\to j}_{\text{msg}} \notin \mathbf{A}^{j}_{\text{intent}},
    \end{aligned} \\
    0, & \text{otherwise.}
    \end{cases}
    \label{eq: per}
\end{equation}

\section{Comparing Cicero to Humans}
\input{tables/dip_benchmark_overall_stat}
% experiment setup 
It is unclear whether \cicero can
achieve human-level gameplay in both tactics and
communication. Having defined the aspects of communication that we
argue are important for mastering \textit{Diplomacy}, we want to
investigate communication and cooperation between \cicero and
humans. Specifically, we want to answer:
\begin{enumerate}
    \item Can \cicero persuade humans?
    \item How deceptive is \cicero compared to humans? 
    \item Can \cicero pass as a human?
\end{enumerate}

We adapt the game engine created by \citet{paquette-19} and introduce
additional measures  to the interface to help us answer these
questions.
%
Following \citet{peskov2020takes}, humans annotate every message that they receive or send for whether it is a lie (truth/lie/neutral options). While \citet{meta2022human} asked \textit{ex post facto} if any opponents were a computer, we inform agents before play that there is a computer and we ask human agents their guess of the humanity of each opposing power.

There are two to four human agents per game (others are \cicero), totaling 69 human agents over all 24 games.
% \footnote{
% We recruit agents from Diplomacy forums and we pay at least \$70 per game, which lasts approximately three hours. We do not collect demographic information.
% }
Games typically finish after fourteen movement turns, where each movement turns is limited to a brisk ten minutes. 
%
%
The game setup differs from Meta's \cicero study: agents in this study know \emph{a priori} that they are playing a bot.
%
%
In total, we collect 27,665 messages from communication between humans
and \cicero (Table~\ref{tab:overall_stat}).
% results

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter Break
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Expert AI Assisting Human Beginners}
\label{ch:expert_AI}
big goal -> then making them into points and connect to pholus and ctrl-d 
\section{PHOLUS}
\label{sec:pholus}
\section{CTRL-D}
\label{sec:ctrld}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Chapter Break
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Proposed Work}
\label{ch:proposal}

\section{topic1}

Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam ac convallis mauris. Etiam interdum accumsan consequat. Quisque at ipsum sit amet urna suscipit laoreet nec sed arcu. Praesent aliquam sit amet eros at vehicula. Integer finibus nunc vitae ligula vestibulum, id tincidunt dolor ultrices. In imperdiet neque et nulla feugiat rutrum non vitae purus. Sed cursus aliquam orci, eu elementum odio sollicitudin in. Phasellus tincidunt nibh non massa lacinia, id egestas enim eleifend. Vestibulum lectus dui, imperdiet eget risus id, euismod placerat tortor. Quisque a sem id velit efficitur iaculis. Proin imperdiet ultrices congue.

\section{topic2}
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nam ac convallis mauris. Etiam interdum accumsan consequat. Quisque at ipsum sit amet urna suscipit laoreet nec sed arcu. Praesent aliquam sit amet eros at vehicula. Integer finibus nunc vitae ligula vestibulum, id tincidunt dolor ultrices. In imperdiet neque et nulla feugiat rutrum non vitae purus. Sed cursus aliquam orci, eu elementum odio sollicitudin in. Phasellus tincidunt nibh non massa lacinia, id egestas enim eleifend. Vestibulum lectus dui, imperdiet eget risus id, euismod placerat tortor. Quisque a sem id velit efficitur iaculis. Proin imperdiet ultrices congue.

\section{Conclusion and Timeline}

\bibliography{bib/custom}
\bibliographystyle{acl_natbib}
\end{document}

